{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = 28\n",
    "input_path = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "train_image_path = join(input_path, 'train-images.idx3-ubyte')\n",
    "train_labels_path = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_image_path = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_path = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "''' \n",
    "image is saved in a byte stream sequentially\n",
    "first 16 for header remaining 28 * 28 pixels of each image in sequence for 60000 images\n",
    "'''\n",
    "\n",
    "with open(train_image_path, 'rb') as file: \n",
    "    data = np.frombuffer(file.read(), dtype = np.uint8)\n",
    "\n",
    "with open(train_labels_path, 'rb') as file:\n",
    "    labels = np.frombuffer(file.read(), dtype = np.uint8)\n",
    "        \n",
    "labels = labels[8:]\n",
    "print(labels.shape)\n",
    "\n",
    "images = data[16:].reshape(-1,image_dim, image_dim) # original size is also (28, 28)\n",
    "print(images.shape)\n",
    "\n",
    "plt.imshow(images[1], cmap=\"gray\")\n",
    "plt.title(\"First Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (48000, 10)\n",
      "(12000, 784) (12000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Dataset Loader\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "def random_rotation(images, max_angle=15):\n",
    "    \"\"\"\n",
    "    Apply random rotations to the images.\n",
    "    :param images: 3D array of images (num_samples, height, width).\n",
    "    :param max_angle: Maximum angle for rotation.\n",
    "    :return: Rotated images.\n",
    "    \"\"\"\n",
    "    rotated_images = []\n",
    "    for image in images:\n",
    "        decision = np.random.rand()\n",
    "        if decision < 0.5: # Apply preprocessing with 50% probability\n",
    "            rotated_images.append(image)\n",
    "        else:\n",
    "            angle = np.random.uniform(-max_angle, max_angle)\n",
    "            rotated_image = rotate(image, angle, reshape=False, mode='nearest')\n",
    "            rotated_images.append(rotated_image)\n",
    "\n",
    "    return np.array(rotated_images)\n",
    "\n",
    "\n",
    "def horizontal_flip(images):\n",
    "    \"\"\"\n",
    "    Apply horizontal flip to the images.\n",
    "    :param images: 3D array of images (num_samples, height, width).\n",
    "    :return: Flipped images.\n",
    "    \"\"\"\n",
    "\n",
    "    flipped_images = []\n",
    "    for image in images:\n",
    "        decision = np.random.rand()\n",
    "        if decision < 0.5: # Apply preprocessing with 50% probability\n",
    "            flipped_images.append(image)\n",
    "        else:\n",
    "            flipped_images.append(np.fliplr(image))\n",
    "            \n",
    "    return np.array(flipped_images)\n",
    "\n",
    "class MNIST_Dataloader:\n",
    "    def __init__(self, train_image_path, train_labels_path, test_image_path, test_labels_path, dim, preprocessors=None):\n",
    "        \"\"\"\n",
    "        Initialize the MNIST dataloader.\n",
    "        :param train_image_path: Path to the training images.\n",
    "        :param train_labels_path: Path to the training labels.\n",
    "        :param test_image_path: Path to the test images.\n",
    "        :param test_labels_path: Path to the test labels.\n",
    "        :param dim: Dimension of the images (e.g., 28 for 28x28 images).\n",
    "        :param preprocessors: List of preprocessing functions to apply to the images.\n",
    "        \"\"\"\n",
    "        self.train_image_path = train_image_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_image_path = test_image_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "        self.dim = dim\n",
    "        self.preprocessors = preprocessors if preprocessors is not None else []\n",
    "\n",
    "    def read_image_labels(self, image_path, label_path, preprocess=False):\n",
    "        with open(image_path, 'rb') as file:\n",
    "            images = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "\n",
    "        # Reshape and normalize images\n",
    "        images = images[16:].reshape(-1, self.dim, self.dim).astype(np.float32)\n",
    "        images /= 255.0  # Normalize to range [0, 1]\n",
    "\n",
    "        if preprocess:\n",
    "            # Apply preprocessing\n",
    "            for preprocessor in self.preprocessors:\n",
    "                images = preprocessor(images)\n",
    "\n",
    "        with open(label_path, 'rb') as file:\n",
    "            labels = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        one_hot_labels = np.eye(10)[labels[8:]]\n",
    "\n",
    "        return images, one_hot_labels\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the MNIST data.\n",
    "        :return: Tuple containing train and test datasets.\n",
    "        \"\"\"\n",
    "        train_images, train_labels = self.read_image_labels(self.train_image_path, self.train_labels_path, True)\n",
    "        test_images, test_labels = self.read_image_labels(self.test_image_path, self.test_labels_path)\n",
    "\n",
    "        # Flatten images back to 1D after preprocessing\n",
    "        train_images = train_images.reshape(-1, self.dim**2)\n",
    "        test_images = test_images.reshape(-1, self.dim**2)\n",
    "\n",
    "        return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "\n",
    "# Creating Loader object to get train, test datasets\n",
    "preprocessors = [random_rotation, horizontal_flip]\n",
    "dobj = MNIST_Dataloader(train_image_path, train_labels_path, test_image_path, test_labels_path, image_dim, preprocessors)\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = dobj.load_data()\n",
    "split_ratio = 0.2\n",
    "split_index = int(len(train_images) * split_ratio)\n",
    "train_images, train_labels = train_images[split_index:], train_labels[split_index:]\n",
    "val_images, val_labels = train_images[:split_index], train_labels[:split_index]\n",
    "\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(val_images.shape, val_labels.shape)\n",
    "print(test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FullyConnectedNN:\n",
    "    def __init__(self, layers, dropout_rate=0.0):\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.best_loss = float('inf')  # Initialize with a high value\n",
    "        \n",
    "        # Initialize weights and biases using He initialization\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / layers[i]))\n",
    "            self.biases.append(np.zeros((1, layers[i + 1])))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        # Check for NaN or Inf in z before applying softmax\n",
    "        if np.any(np.isnan(z)) or np.any(np.isinf(z)):\n",
    "            print(\"Warning: NaN or Inf detected in z before softmax.\")\n",
    "        z_stable = z - np.max(z, axis=1, keepdims=True)  # Stability improvement\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def apply_dropout(self, activations, dropout_rate):\n",
    "        if dropout_rate > 0:\n",
    "            assert 0 <= dropout_rate < 1.0, \"Dropout rate must be between 0 and 1 (exclusive).\"\n",
    "            mask = np.random.rand(*activations.shape) > dropout_rate\n",
    "            activations *= mask\n",
    "            activations /= (1 - dropout_rate)  # Scale to maintain consistency\n",
    "            return activations, mask\n",
    "        return activations, None\n",
    "\n",
    "    def feedforward(self, X, training=True):\n",
    "        activations = [X]\n",
    "        self.dropout_masks = []  # Store dropout masks for backpropagation\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            if i == len(self.weights) - 1:  # Output layer\n",
    "                a = self.softmax(z)\n",
    "            else:  # Hidden layers\n",
    "                a = self.relu(z)\n",
    "                if training:  # Apply dropout during training\n",
    "                    a, mask = self.apply_dropout(a, self.dropout_rate)\n",
    "                    self.dropout_masks.append(mask)\n",
    "                else:\n",
    "                    self.dropout_masks.append(None)  # No dropout during inference\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        activations = self.feedforward(X, training=True)\n",
    "        deltas = [activations[-1] - y]\n",
    "\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[-1], self.weights[i].T)\n",
    "            if self.dropout_masks[i - 1] is not None:\n",
    "                delta *= self.dropout_masks[i - 1]  # Apply dropout mask\n",
    "            delta *= self.relu_derivative(activations[i])\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse()\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update weights and biases with gradient clipping\n",
    "            grad_w = np.dot(activations[i].T, deltas[i])\n",
    "            grad_b = np.sum(deltas[i], axis=0, keepdims=True)\n",
    "            grad_w = np.clip(grad_w, -2.0, 2.0)  # Clip gradients to prevent explosion\n",
    "            grad_b = np.clip(grad_b, -2.0, 2.0)\n",
    "            self.weights[i] -= learning_rate * grad_w\n",
    "            self.biases[i] -= learning_rate * grad_b\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            self.backpropagation(X, y, learning_rate)\n",
    "            train_loss = self.calculate_loss(X, y)\n",
    "            val_loss = self.calculate_loss(X_val, y_val)\n",
    "            accuracy = self.calculate_accuracy(X_val, y_val)\n",
    "\n",
    "            # Save the best weights based on validation loss\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                best_weights = [w.copy() for w in self.weights]\n",
    "                best_biases = [b.copy() for b in self.biases]\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "                self.save_best_weights('best_weights.npy', best_weights, best_biases)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.feedforward(X, training=False)[-1]\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        # Add epsilon to avoid log(0)\n",
    "        return -np.mean(np.sum(y * np.log(predictions + 1e-8), axis=1))\n",
    "\n",
    "    def calculate_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        return np.mean(predicted_labels == true_labels)\n",
    "\n",
    "    def save_best_weights(self, filepath, best_weights, best_biases):\n",
    "        np.save(filepath, {'weights': best_weights, 'biases': best_biases})\n",
    "        print(\"Best weights saved to disk.\")\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        data = np.load(filepath, allow_pickle=True).item()\n",
    "        self.weights = data['weights']\n",
    "        self.biases = data['biases']\n",
    "        print(\"Best weights loaded from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 848128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m nn \u001b[38;5;241m=\u001b[39m FullyConnectedNN(layers, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the network (use train_images and train_labels)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 80\u001b[0m, in \u001b[0;36mFullyConnectedNN.train\u001b[1;34m(self, X, y, X_val, y_val, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, X_val, y_val, epochs, learning_rate):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(X, y)\n\u001b[0;32m     82\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_loss(X_val, y_val)\n",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m, in \u001b[0;36mFullyConnectedNN.backpropagation\u001b[1;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackpropagation\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, learning_rate):\n\u001b[1;32m---> 57\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     deltas \u001b[38;5;241m=\u001b[39m [activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y]\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[9], line 43\u001b[0m, in \u001b[0;36mFullyConnectedNN.feedforward\u001b[1;34m(self, X, training)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_masks \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Store dropout masks for backpropagation\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n\u001b[1;32m---> 43\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Output layer\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(z)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define network structure: 784 input neurons, 1 hidden layer with 256 neurons each, 2 hidden layer with 128 neurons and 10 output neuron\n",
    "layers = [image_dim ** 2, 256, 128, 10]\n",
    "\n",
    "# Calculate the number of trainable parameters in the network\n",
    "params = 0\n",
    "for i in range(len(layers) - 1): \n",
    "    if i == 0:\n",
    "        params += layers[i] * image_dim ** 2\n",
    "    else:\n",
    "        params += layers[i] * layers[i - 1]\n",
    "\n",
    "print(\"Total trainable parameters:\", params)\n",
    "\n",
    "# Create the neural network\n",
    "nn = FullyConnectedNN(layers, 0.2)\n",
    "\n",
    "# Train the network (use train_images and train_labels)\n",
    "nn.train(train_images, train_labels, val_images, val_labels, epochs=5000, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights loaded from disk.\n",
      "Test Accuracy: 96.88%\n",
      "Test Loss: 0.1649906022314226\n"
     ]
    }
   ],
   "source": [
    "layers = [image_dim ** 2, 256, 128, 10]\n",
    "\n",
    "nn = FullyConnectedNN(layers, 0.2)\n",
    "\n",
    "nn.load_weights('best_weights.npy')\n",
    "accuracy = nn.calculate_accuracy(test_images, test_labels)\n",
    "loss = nn.calculate_loss(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
